{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from IPython.display import display\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_train = pd.read_parquet('energy_train.parquet')\n",
    "energy_test2 = pd.read_parquet('energy_test2.parquet')\n",
    "energy_test1 = pd.read_parquet('energy_test1.parquet')\n",
    "forecasts = pd.read_parquet('forecasts.parquet')\n",
    "\n",
    "energy_test1_copy = pd.read_parquet('energy_test1.parquet')\n",
    "energy_test1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenführen der Wettermodelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['DWD ICON', 'NCEP GFS']:\n",
    "    # Filter für das Wettermodell\n",
    "    forecasts_model = forecasts[forecasts['Weather Model'] == model].copy()\n",
    "    \n",
    "    # Spalten umbenennen\n",
    "    forecasts_model = forecasts_model.rename(columns={\n",
    "        'SolarDownwardRadiation': f'SolarDownwardRadiation_{model.replace(\" \", \"_\")}',\n",
    "        'CloudCover': f'CloudCover_{model.replace(\" \", \"_\")}',\n",
    "        'Temperature': f'Temperature_{model.replace(\" \", \"_\")}'\n",
    "    })\n",
    "    \n",
    "    # 'valid_datetime' berechnen\n",
    "    forecasts_model['valid_datetime'] = pd.to_datetime(forecasts_model['ref_datetime']) + pd.to_timedelta(forecasts_model['valid_time'], unit='h')\n",
    "    \n",
    "    # Forecast DataFrame für das spezifische Modell speichern\n",
    "    if model == 'DWD ICON':\n",
    "        forecasts_dwd = forecasts_model\n",
    "    else:\n",
    "        forecasts_ncep = forecasts_model\n",
    "\n",
    "# Zusammenführen der beiden Modelle\n",
    "forecasts_combined = pd.merge(\n",
    "    forecasts_ncep,\n",
    "    forecasts_dwd, \n",
    "    on=['ref_datetime', 'valid_time', 'valid_datetime'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "forecasts_combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenführen der Energiedaten und Wettervorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging beide DataFrames basierend auf den Spalten 'dtm' und 'ref_datetime' in energy_train\n",
    "# sowie 'valid_datetime' und 'ref_datetime' in forecasts_combined (inner join).\n",
    "energy_train_mit_forecast = pd.merge(\n",
    "    energy_train, \n",
    "    forecasts_combined, \n",
    "    left_on=['dtm', 'ref_datetime'], \n",
    "    right_on=['valid_datetime', 'ref_datetime'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Entferne Zeilen, bei denen die Zielvariable 'Solar_MWh' NaN ist.\n",
    "energy_train_mit_forecast = energy_train_mit_forecast[energy_train_mit_forecast[\"Solar_MWh\"].isna() == False]\n",
    "\n",
    "energy_train_mit_forecast.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere Monat und Jahr aus der Spalte 'dtm' im Format \"Monat Jahr\"\n",
    "energy_train_mit_forecast['month_year'] = energy_train_mit_forecast['dtm'].dt.strftime('%B %Y')\n",
    "\n",
    "# Erhalte eindeutige Werte der Monate und Jahre\n",
    "unique_months_years = energy_train_mit_forecast['month_year'].unique()\n",
    "\n",
    "print(unique_months_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generieren neuer Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korrelation mit Solar_MWh(Aus Aufgabe 4)\n",
    "\n",
    "| Feature                                | Korrelation |\n",
    "|----------------------------------------|-------------|\n",
    "| SolarDownwardRadiation_DWD_ICON        | 0.952156    |\n",
    "| SolarDownwardRadiation_NCEP_GFS        | 0.933041    |\n",
    "| radiation_temp_interaction_DWD_ICON   | 0.866000    |\n",
    "| radiation_temp_interaction_NCEP_GFS   | 0.853576    |\n",
    "| CloudRadiationLoss                     | 0.828186    |\n",
    "| effective_radiation_NCEP_GFS          | 0.779750    |\n",
    "| effective_radiation_DWD_ICON          | 0.724797    |\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_train_mit_forecast['time'] = energy_train_mit_forecast['dtm'].dt.time  #Stunden extrahieren \n",
    "energy_train_mit_forecast['CloudRadiationLoss'] =(energy_train_mit_forecast['SolarDownwardRadiation_DWD_ICON']* energy_train_mit_forecast['CloudCover_DWD_ICON'])\n",
    "energy_train_mit_forecast['effective_radiation_DWD_ICON'] = energy_train_mit_forecast['SolarDownwardRadiation_DWD_ICON'] * (1 - energy_train_mit_forecast['CloudCover_DWD_ICON'])\n",
    "energy_train_mit_forecast['effective_radiation_NCEP_GFS'] = energy_train_mit_forecast['SolarDownwardRadiation_NCEP_GFS'] * (1 - energy_train_mit_forecast['CloudCover_NCEP_GFS'])\n",
    "energy_train_mit_forecast['radiation_temp_interaction_DWD_ICON'] = energy_train_mit_forecast['SolarDownwardRadiation_DWD_ICON'] * energy_train_mit_forecast['Temperature_DWD_ICON'] \n",
    "energy_train_mit_forecast['radiation_temp_interaction_NCEP_GFS'] = energy_train_mit_forecast['SolarDownwardRadiation_NCEP_GFS'] * energy_train_mit_forecast['Temperature_NCEP_GFS'] \n",
    "energy_train_mit_forecast.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen von Trainings- und Validierungsset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere Monat und Jahr zur Filterung\n",
    "energy_train_mit_forecast['month_year'] = energy_train_mit_forecast['dtm'].dt.to_period('M')\n",
    "\n",
    "# Teile die Daten in Trainings- und Validierungssets auf\n",
    "# Das Trainings-Set umfasst die Daten mit `month_year` zwischen September 2020 und Juni 2022 (inklusive).\n",
    "df_train = energy_train_mit_forecast[(energy_train_mit_forecast['month_year'] >= '2020-09') & (energy_train_mit_forecast['month_year'] <= '2022-06')]\n",
    "\n",
    "# Das Test-Set umfasst die Daten mit `month_year` nach Juni 2022.\n",
    "df_test = energy_train_mit_forecast[energy_train_mit_forecast['month_year'] > '2022-06']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zu entfernende Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der zu entfernenden Spalten\n",
    "columns_to_drop = [\"dtm\", \"ref_datetime\", \"Weather Model_x\", \"Weather Model_y\", \"valid_datetime\", \"valid_time\", 'month_year',\n",
    "                   \"CloudCover_NCEP_GFS\", \"CloudCover_DWD_ICON\", \"Temperature_NCEP_GFS\", \"Temperature_DWD_ICON\", \"Solar_capacity_mwp\"] \n",
    "\n",
    "# Entfernen der definierten Spalten\n",
    "df_train = df_train.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"time\"] \n",
    "numerical_features = [\"SolarDownwardRadiation_DWD_ICON\", \"SolarDownwardRadiation_NCEP_GFS\", \"CloudRadiationLoss\", \"effective_radiation_DWD_ICON\",\n",
    "                     \"effective_radiation_NCEP_GFS\", \"radiation_temp_interaction_DWD_ICON\", \"radiation_temp_interaction_NCEP_GFS\"  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen des Labels aus den Daten\n",
    "y_train= df_train.pop(\"Solar_MWh\")\n",
    "y_test= df_test.pop(\"Solar_MWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columntransformer and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ziel: Vorverarbeitung der Daten (kategoriale und numerische Features) in separaten Pipelines.\n",
    "column_trans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # 1. Vorverarbeitung der kategorialen Features:\n",
    "        #    a) Fehlende Werte (NaN) in kategorialen Spalten durch die häufigste Kategorie ersetzen (\"most_frequent\").\n",
    "        #    b) One-hot Encoding für kategoriale Features, um sie in numerische Werte zu transformieren.\n",
    "        (\"onehot\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),  # Fehlende Werte ersetzen\n",
    "            (\"encode\", OneHotEncoder(handle_unknown=\"ignore\"))  # One-hot Encoding\n",
    "        ]), categorical_features),  # Liste der kategorialen Features\n",
    "        \n",
    "        # 2. Vorverarbeitung der numerischen Features:\n",
    "        #    a) Skalierung der numerischen Spalten, um sie standardisiert (Mittelwert=0, Varianz=1) darzustellen.\n",
    "        (\"impute_scale\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"mean\")),   # Fehlende Werte ersetzen\n",
    "            (\"scale\", StandardScaler())  # Skalierung der numerischen Daten\n",
    "        ]), numerical_features)  # Liste der numerischen Features\n",
    "    ],\n",
    "    # Alle anderen Spalten werden unverändert beibehalten (falls vorhanden), da `remainder=\"passthrough\"` angegeben ist.\n",
    "    remainder=\"passthrough\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfache lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer Pipeline mit Preprocessing und LinearRegression-Modell\n",
    "linear_pipe = Pipeline([\n",
    "    ('preprocessing', column_trans),  # Vorverarbeitung der Eingabedaten\n",
    "    ('model', LinearRegression())    # Lineares Regressionsmodell\n",
    "])\n",
    "\n",
    "# Parameter-Grid für das Modell (leer, da keine Hyperparameter optimiert werden)\n",
    "linear_param_grid = {}\n",
    "\n",
    "# GridSearchCV zur Optimierung des Modells\n",
    "linear_gs = GridSearchCV(\n",
    "    estimator=linear_pipe,                    # Pipeline als Estimator\n",
    "    param_grid=linear_param_grid,             # Parameter-Grid\n",
    "    scoring='neg_root_mean_squared_error',    # Bewertungsmetrik (negativer RMSE)\n",
    "    cv=5,                                     # Kreuzvalidierung mit 5 Folds\n",
    "    n_jobs=-1                                 # Parallele Verarbeitung\n",
    ")\n",
    "\n",
    "# Fit des Modells auf die Trainingsdaten\n",
    "linear_gs.fit(X_train, y_train)\n",
    "\n",
    "# Bestes Modell und Trainings-RMSE (basierend auf Kreuzvalidierung)\n",
    "linear_best_model = linear_gs.best_estimator_\n",
    "linear_train_rmse = np.sqrt(-linear_gs.best_score_)\n",
    "\n",
    "# Vorhersage auf dem Test-Datensatz und Berechnung des Test-RMSE\n",
    "y_pred = linear_best_model.predict(X_test)\n",
    "linear_test_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "print(\"Bestes RMSE auf dem Trainingsdatensatz (Kreuzvalidierung):\", linear_train_rmse)\n",
    "print(\"RMSE auf dem Testdatensatz:\", linear_test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die wichtigsten Features (Einfache lineare Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf das LinearRegression-Modell aus der Pipeline\n",
    "linear_model = linear_best_model.named_steps['model']\n",
    "\n",
    "# Extrahieren der Feature-Namen aus dem Preprocessing-Schritt\n",
    "if hasattr(linear_best_model.named_steps['preprocessing'], 'get_feature_names_out'):\n",
    "    feature_names = linear_best_model.named_steps['preprocessing'].get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "# Erstellung eines DataFrames mit den Koeffizienten und den entsprechenden Feature-Namen\n",
    "coefficients_linear = pd.DataFrame({\n",
    "    \"Feature Name\": feature_names,\n",
    "    \"Coefficient\": linear_model.coef_\n",
    "})\n",
    "\n",
    "# Sortieren der Koeffizienten nach ihrem absoluten Wert in absteigender Reihenfolge\n",
    "coefficients_linear_sorted = coefficients_linear.sort_values(\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "# Ausgabe der Top-Features nach der Höhe ihrer Koeffizienten\n",
    "print(coefficients_linear_sorted.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer Pipeline mit Preprocessing und Ridge-Regression\n",
    "ridge_pipe = Pipeline([\n",
    "    ('preprocessing', column_trans),  # Vorverarbeitung der Daten\n",
    "    ('model', Ridge())                # Ridge-Regression\n",
    "])\n",
    "\n",
    "# Parameter-Grid für die Ridge-Regression (Optimierung von Alpha(lambda))\n",
    "ridge_param_grid = {\"model__alpha\": [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "# GridSearchCV zur Optimierung des Modells\n",
    "ridge_gs = GridSearchCV(\n",
    "    estimator=ridge_pipe,                    # Pipeline als Estimator\n",
    "    param_grid=ridge_param_grid,             # Parameter-Grid\n",
    "    scoring='neg_root_mean_squared_error',   # Bewertungsmetrik (negativer RMSE)\n",
    "    cv=5,                                    # Kreuzvalidierung mit 5 Folds\n",
    "    n_jobs=-1                                # Parallele Verarbeitung\n",
    ")\n",
    "\n",
    "# Fit des Modells auf die Trainingsdaten\n",
    "ridge_gs.fit(X_train, y_train)\n",
    "\n",
    "# Bestes Modell, beste Parameter und Trainings-RMSE (basierend auf Kreuzvalidierung)\n",
    "ridge_best_params = ridge_gs.best_params_\n",
    "ridge_best_model = ridge_gs.best_estimator_\n",
    "ridge_train_rmse = np.sqrt(-ridge_gs.best_score_)\n",
    "\n",
    "# Vorhersage auf dem Test-Datensatz und Berechnung des Test-RMSE\n",
    "y_pred = ridge_best_model.predict(X_test)\n",
    "ridge_test_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "print(\"Beste Parameter:\", ridge_best_params)\n",
    "print(\"Bestes RMSE auf dem Trainingsdatensatz (Kreuzvalidierung):\", ridge_train_rmse)\n",
    "print(\"RMSE auf dem Testdatensatz:\", ridge_test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die wichtigsten Features (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf das Ridge-Modell aus der Pipeline\n",
    "ridge_model = ridge_best_model.named_steps['model']\n",
    "\n",
    "# Extrahieren der Feature-Namen aus dem Preprocessing-Schritt\n",
    "if hasattr(ridge_best_model.named_steps['preprocessing'], 'get_feature_names_out'):\n",
    "    feature_names = ridge_best_model.named_steps['preprocessing'].get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "# Erstellung eines DataFrames mit den Koeffizienten und den entsprechenden Feature-Namen\n",
    "coefficients_ridge = pd.DataFrame({\n",
    "    \"Feature Name\": feature_names,\n",
    "    \"Coefficient\": ridge_model.coef_\n",
    "})\n",
    "\n",
    "# Sortieren der Koeffizienten nach ihrem absoluten Wert in absteigender Reihenfolge\n",
    "coefficients_ridge_sorted = coefficients_ridge.sort_values(\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "# Ausgabe der Top-Features nach der Höhe ihrer Koeffizienten\n",
    "print(coefficients_ridge_sorted.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer Pipeline mit Preprocessing und Lasso-Regression\n",
    "lasso_pipe = Pipeline([\n",
    "    ('preprocessing', column_trans),  # Vorverarbeitung der Eingabedaten\n",
    "    ('model', Lasso())                # Lasso-Regression\n",
    "])\n",
    "\n",
    "# Parameter-Grid für die Lasso-Regression (Optimierung von Alpha(lambda))\n",
    "lasso_param_grid = {\n",
    "    \"model__alpha\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV zur Optimierung des Modells\n",
    "lasso_gs = GridSearchCV(\n",
    "    estimator=lasso_pipe,                    # Pipeline als Estimator\n",
    "    param_grid=lasso_param_grid,             # Parameter-Grid\n",
    "    scoring='neg_root_mean_squared_error',   # Bewertungsmetrik (negativer RMSE)\n",
    "    cv=5,                                    # Kreuzvalidierung mit 5 Folds\n",
    "    n_jobs=-1                                # Parallele Verarbeitung\n",
    ")\n",
    "\n",
    "# Fit des Modells auf die Trainingsdaten\n",
    "lasso_gs.fit(X_train, y_train)\n",
    "\n",
    "# Bestes Modell, beste Parameter und Trainings-RMSE (basierend auf Kreuzvalidierung)\n",
    "lasso_best_params = lasso_gs.best_params_\n",
    "lasso_best_model = lasso_gs.best_estimator_\n",
    "lasso_train_rmse = np.sqrt(-lasso_gs.best_score_)\n",
    "\n",
    "# Vorhersage auf dem Test-Datensatz und Berechnung des Test-RMSE\n",
    "y_pred = lasso_best_model.predict(X_test)\n",
    "lasso_test_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "print(\"Beste Parameter:\", lasso_best_params)\n",
    "print(\"Bestes RMSE auf dem Trainingsdatensatz (Kreuzvalidierung):\", lasso_train_rmse)\n",
    "print(\"RMSE auf dem Testdatensatz:\", lasso_test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die wichtigsten Features (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf das Lasso-Modell aus der Pipeline\n",
    "lasso_model = lasso_best_model.named_steps['model']\n",
    "\n",
    "# Extrahieren der Feature-Namen aus dem Preprocessing-Schritt\n",
    "if hasattr(lasso_best_model.named_steps['preprocessing'], 'get_feature_names_out'):\n",
    "    feature_names = lasso_best_model.named_steps['preprocessing'].get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "# Erstellung eines DataFrames mit den Koeffizienten und den entsprechenden Feature-Namen\n",
    "coefficients_lasso = pd.DataFrame({\n",
    "    \"Feature Name\": feature_names,\n",
    "    \"Coefficient\": lasso_model.coef_\n",
    "})\n",
    "\n",
    "# Filtern der Features mit Koeffizienten ungleich null (nicht eliminierte Features)\n",
    "non_zero_coefficients = coefficients_lasso[coefficients_lasso[\"Coefficient\"] != 0]\n",
    "\n",
    "# Sortieren der nicht-null Koeffizienten nach ihrem absoluten Wert in absteigender Reihenfolge\n",
    "coefficients_lasso_sorted = non_zero_coefficients.sort_values(\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "# Ausgabe der Top-Features nach der Höhe ihrer Koeffizienten\n",
    "print(coefficients_lasso_sorted.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entscheidungsbaum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer Pipeline mit Preprocessing und DecisionTreeRegressor\n",
    "dt_pipe = Pipeline([\n",
    "    ('preprocessing', column_trans),              # Vorverarbeitung der Eingabedaten\n",
    "    ('model', DecisionTreeRegressor(random_state=42))  # Entscheidungsbaum-Regressor\n",
    "])\n",
    "\n",
    "# Parameter-Grid für den Entscheidungsbaum (maximale Tiefe und minimale Anzahl an Proben für eine Teilung)\n",
    "dt_param_grid = {\n",
    "    \"model__max_depth\": [3, 5, 7, 10, 20, 25, 50, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 15, 20, 30, 50, 100]\n",
    "}\n",
    "\n",
    "# GridSearchCV zur Optimierung des Modells\n",
    "dt_gs = GridSearchCV(\n",
    "    estimator=dt_pipe,                    # Pipeline als Estimator\n",
    "    param_grid=dt_param_grid,             # Parameter-Grid\n",
    "    scoring='neg_root_mean_squared_error',  # Bewertungsmetrik (negativer RMSE)\n",
    "    cv=5,                                 # Kreuzvalidierung mit 5 Folds\n",
    "    n_jobs=-1                             # Parallele Verarbeitung\n",
    ")\n",
    "\n",
    "# Fit des Modells auf die Trainingsdaten\n",
    "dt_gs.fit(X_train, y_train)\n",
    "\n",
    "# Bestes Modell, beste Parameter und Trainings-RMSE (basierend auf Kreuzvalidierung)\n",
    "dt_best_params = dt_gs.best_params_\n",
    "dt_best_model = dt_gs.best_estimator_\n",
    "dt_train_rmse = np.sqrt(-dt_gs.best_score_)\n",
    "\n",
    "# Ausgabe der besten Parameter und des RMSE auf dem Trainingsdatensatz\n",
    "print(\"Decision Tree Best Parameters:\", dt_best_params)\n",
    "print(\"Decision Tree Best RMSE (Cross-Validation):\", dt_train_rmse)\n",
    "\n",
    "# Vorhersage auf dem Test-Datensatz und Berechnung des Test-RMSE\n",
    "y_pred = dt_best_model.predict(X_test)\n",
    "dt_test_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Ausgabe des Test-RMSE\n",
    "print(\"\\nTest RMSE:\", dt_test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die wichtigsten Features (Entscheidungsbaum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf das Decision Tree-Modell aus der Pipeline\n",
    "dt_model = dt_best_model.named_steps['model']\n",
    "\n",
    "# Extrahieren der Feature-Namen aus dem Preprocessing-Schritt\n",
    "if hasattr(dt_best_model.named_steps['preprocessing'], 'get_feature_names_out'):\n",
    "    feature_names = dt_best_model.named_steps['preprocessing'].get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "# Visualisierung des Entscheidungsbaums\n",
    "plt.figure(figsize=(20, 10))  # Festlegen der Größe der Grafik\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=feature_names,  # Anzeige der Feature-Namen\n",
    "    rounded=True,                 # Abgerundete Knoten\n",
    "    filled=True,                  # Farbige Knoten basierend auf den Werten\n",
    "    fontsize=10                   # Schriftgröße\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\", fontsize=16)  # Titel der Grafik\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble-Modell(Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer Pipeline mit Preprocessing und GradientBoostingRegressor\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", column_trans),                     # Vorverarbeitung der Eingabedaten\n",
    "    (\"model\", GradientBoostingRegressor(random_state=42))  # Gradient Boosting Regressor\n",
    "])\n",
    "\n",
    "# Parameter-Grid für die Optimierung des Modells\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.2],             # Lernrate\n",
    "    \"model__n_estimators\": [100, 200, 500],               # Anzahl der Bäume\n",
    "    \"model__max_depth\": [3, 5, 10],                       # Maximale Tiefe der Bäume\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None],        # Anzahl der zu verwendenden Features\n",
    "    \"preprocessing__onehot__impute__strategy\": [\"most_frequent\"],  #  für kategorische Variablen\n",
    "    \"preprocessing__impute_scale__impute__strategy\": [\"mean\"]    # für numerische Variablen\n",
    "}\n",
    "\n",
    "# GridSearchCV zur Optimierung des Modells\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit des Modells auf die Trainingsdaten\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Bestes Modell, beste Parameter und Trainings-RMSE (basierend auf Kreuzvalidierung)\n",
    "best_params = gs.best_params_\n",
    "best_model = gs.best_estimator_\n",
    "train_rmse = np.sqrt(-gs.best_score_)\n",
    "\n",
    "# Ausgabe der besten Parameter und des RMSE auf dem Trainingsdatensatz\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best RMSE (Cross-Validation):\", train_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage auf dem Test-Datensatz\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Berechnung des RMSE auf dem Test-Datensatz\n",
    "test_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best RMSE on Training Set (Cross-Validation):\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die wichtigsten Features (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf das beste Modell aus der Pipeline\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# Zugriff auf den GradientBoostingRegressor aus der Pipeline\n",
    "gbr_model = best_model.named_steps['model']\n",
    "\n",
    "# Extrahieren der Feature-Namen aus dem Preprocessing-Schritt\n",
    "if hasattr(best_model.named_steps['preprocessing'], 'get_feature_names_out'):\n",
    "    feature_names = best_model.named_steps['preprocessing'].get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "# Zugriff auf die Feature-Importances des GradientBoostingRegressor\n",
    "feature_importances = gbr_model.feature_importances_\n",
    "\n",
    "# Erstellung eines DataFrames mit den Importances und den entsprechenden Feature-Namen\n",
    "importances_df = pd.DataFrame({\n",
    "    \"Feature Name\": feature_names,\n",
    "    \"Importance\": feature_importances\n",
    "})\n",
    "\n",
    "# Sortieren der Features nach ihrer Wichtigkeit in absteigender Reihenfolge\n",
    "importances_sorted = importances_df.sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "# Ausgabe der Top-Features\n",
    "print(\"Top-Features nach Wichtigkeit:\")\n",
    "print(importances_sorted.head())\n",
    "\n",
    "# Optional: Visualisierung der Feature-Importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances_sorted[\"Feature Name\"].head(10), importances_sorted[\"Importance\"].head(10))\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top 10 wichtigste Features\")\n",
    "plt.gca().invert_yaxis()  # Damit die wichtigsten Features oben stehen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
