{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from IPython.display import display\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_train = pd.read_parquet('energy_train.parquet')\n",
    "energy_test2 = pd.read_parquet('energy_test2.parquet')\n",
    "energy_test1 = pd.read_parquet('energy_test1.parquet')\n",
    "forecasts = pd.read_parquet('forecasts.parquet')\n",
    "\n",
    "energy_test1_copy = pd.read_parquet('energy_test1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenführen der Wettermodelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['DWD ICON', 'NCEP GFS']:\n",
    "    # Filter für das Wettermodell\n",
    "    forecasts_model = forecasts[forecasts['Weather Model'] == model].copy()\n",
    "    \n",
    "    # Spalten umbenennen\n",
    "    forecasts_model = forecasts_model.rename(columns={\n",
    "        'SolarDownwardRadiation': f'SolarDownwardRadiation_{model.replace(\" \", \"_\")}',\n",
    "        'CloudCover': f'CloudCover_{model.replace(\" \", \"_\")}',\n",
    "        'Temperature': f'Temperature_{model.replace(\" \", \"_\")}'\n",
    "    })\n",
    "    \n",
    "    # 'valid_datetime' berechnen\n",
    "    forecasts_model['valid_datetime'] = pd.to_datetime(forecasts_model['ref_datetime']) + pd.to_timedelta(forecasts_model['valid_time'], unit='h')\n",
    "    \n",
    "    # Forecast DataFrame für das spezifische Modell speichern\n",
    "    if model == 'DWD ICON':\n",
    "        forecasts_dwd = forecasts_model\n",
    "    else:\n",
    "        forecasts_ncep = forecasts_model\n",
    "\n",
    "# Zusammenführen der beiden Modelle\n",
    "forecasts_combined = pd.merge(\n",
    "    forecasts_ncep,\n",
    "    forecasts_dwd, \n",
    "    on=['ref_datetime', 'valid_time', 'valid_datetime'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "forecasts_combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenführen der Energiedaten und Wettervorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge beide DataFrames basierend auf den Spalten 'dtm' und 'ref_datetime' in energy_train\n",
    "# sowie 'valid_datetime' und 'ref_datetime' in forecasts_combined (inner join).\n",
    "energy_train_mit_forecast = pd.merge(\n",
    "    energy_train, \n",
    "    forecasts_combined, \n",
    "    left_on=['dtm', 'ref_datetime'], \n",
    "    right_on=['valid_datetime', 'ref_datetime'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Entferne Zeilen, bei denen die Zielvariable 'Solar_MWh' NaN ist.\n",
    "energy_train_mit_forecast = energy_train_mit_forecast[energy_train_mit_forecast[\"Solar_MWh\"].isna() == False]\n",
    "\n",
    "energy_train_mit_forecast.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere Monat und Jahr aus der Spalte 'dtm' im Format \"Monat Jahr\"\n",
    "energy_train_mit_forecast['month_year'] = energy_train_mit_forecast['dtm'].dt.strftime('%B %Y')\n",
    "\n",
    "# Erhalte eindeutige Werte der Monate und Jahre\n",
    "unique_months_years = energy_train_mit_forecast['month_year'].unique()\n",
    "\n",
    "print(unique_months_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenführen der Testdaten und Wettervorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge energy_test1 und forecasts_combined basierend auf 'dtm' und 'valid_datetime' (left join)\n",
    "energy_test1_mit_forecast = pd.merge(\n",
    "    energy_test1,\n",
    "    forecasts_combined,\n",
    "    left_on='dtm',\n",
    "    right_on='valid_datetime',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Behalte nur Zeilen, bei denen 'ref_datetime_y' (forecasts_combined) <= 'ref_datetime_x' (energy_test1)\n",
    "energy_test1_mit_forecast = energy_test1_mit_forecast[energy_test1_mit_forecast['ref_datetime_y'] <= energy_test1_mit_forecast['ref_datetime_x']]\n",
    "\n",
    "# Für jede 'dtm'-Zeile: Behalte die Zeile mit dem neuesten 'ref_datetime_y' (höchster Wert)\n",
    "energy_test1_mit_forecast = energy_test1_mit_forecast.loc[energy_test1_mit_forecast.groupby('dtm')['ref_datetime_y'].idxmax()]\n",
    "\n",
    "# Anzahl der fehlenden Werte in 'valid_datetime' ausgeben\n",
    "print(energy_test1_mit_forecast['valid_datetime'].isnull().sum())\n",
    "\n",
    "# Übersicht der DataFrame-Struktur und Spalteninformationen anzeigen\n",
    "energy_test1_mit_forecast.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Nur für Test 2) Finde die fehlenden Zeilen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifiziere die Zeilen in `energy_test1`, deren `dtm`-Werte nicht in der Spalte `dtm` von `energy_test1_mit_forecast` enthalten sind. \n",
    "\n",
    "#missing_rows = energy_test1[~energy_test1['dtm'].isin(energy_test1_mit_forecast['dtm'])]\n",
    "\n",
    "#print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#energy_test1_copy = energy_test1_copy[energy_test1_copy['dtm'].isin(energy_test1_mit_forecast['dtm'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welche Daten werden von energy_test1_mit_forecast abgedeckt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere Monat und Jahr aus der Spalte 'dtm' im Format \"Monat Jahr\"\n",
    "energy_test1_mit_forecast['month_year'] = energy_test1_mit_forecast['dtm'].dt.strftime('%B %Y')\n",
    "\n",
    "# Erhalte eindeutige Werte der Monate und Jahre\n",
    "unique_months_years = energy_test1_mit_forecast['month_year'].unique()\n",
    "\n",
    "print(unique_months_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generieren neuer Features(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_train_mit_forecast['time'] = energy_train_mit_forecast['dtm'].dt.time  #Stunden extrahieren\n",
    "energy_train_mit_forecast['effective_radiation'] = energy_train_mit_forecast['SolarDownwardRadiation_DWD_ICON'] * (1 - energy_train_mit_forecast['CloudCover_DWD_ICON'])\n",
    "energy_train_mit_forecast.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generieren neuer Features(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_test1_mit_forecast['time'] = energy_test1_mit_forecast['dtm'].dt.time  #Stunden extrahieren\n",
    "energy_test1_mit_forecast['effective_radiation'] = energy_test1_mit_forecast['SolarDownwardRadiation_DWD_ICON'] * (1 - energy_test1_mit_forecast['CloudCover_DWD_ICON'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behandlung von fehlenden Datensätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gruppiere die Daten nach der Spalte 'time'. Dies teilt die Daten in Gruppen basierend auf gleichen Zeitwerten.\n",
    "# 2. Wende innerhalb jeder Gruppe eine \"forward fill\" (ffill) Methode an. Diese Methode füllt fehlende Werte\n",
    "#    (NaN) durch den zuletzt bekannten Wert aus der vorhergehenden Zeile innerhalb der Gruppe.\n",
    "energy_train_mit_forecast['SolarDownwardRadiation_DWD_ICON'] = (\n",
    "    energy_train_mit_forecast.groupby('time')['SolarDownwardRadiation_DWD_ICON']\n",
    "    .ffill()\n",
    ")\n",
    "\n",
    "energy_train_mit_forecast['SolarDownwardRadiation_NCEP_GFS'] = (\n",
    "    energy_train_mit_forecast.groupby('time')['SolarDownwardRadiation_NCEP_GFS']\n",
    "    .ffill()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gruppiere die Daten nach der Spalte 'time'. Dies teilt die Daten in Gruppen basierend auf gleichen Zeitwerten.\n",
    "# 2. Wende innerhalb jeder Gruppe eine \"forward fill\" (ffill) Methode an. Diese Methode füllt fehlende Werte\n",
    "#    (NaN) durch den zuletzt bekannten Wert aus der vorhergehenden Zeile innerhalb der Gruppe.\n",
    "energy_test1_mit_forecast['SolarDownwardRadiation_DWD_ICON'] = (\n",
    "    energy_test1_mit_forecast.groupby('time')['SolarDownwardRadiation_DWD_ICON']\n",
    "    .ffill()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#energy_train_mit_forecast = energy_train_mit_forecast[~energy_train_mit_forecast['month'].isin(['November', 'December'])]\n",
    "#energy_test1_mit_forecast = energy_test1_mit_forecast[~energy_test1_mit_forecast['month'].isin(['November', 'December'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung des DataFrames in Tag- und Nachtzeiten basierend auf Solar_MWh und SolarDownwardRadiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedingung: Nachtzeit \n",
    "night_condition = (energy_train_mit_forecast['Solar_MWh'] == 0) & (energy_train_mit_forecast['SolarDownwardRadiation_NCEP_GFS'] == 0)\n",
    "\n",
    "# Daten für die Nachtzeit filtern\n",
    "night = energy_train_mit_forecast[night_condition]\n",
    "\n",
    "# Daten für die Tageszeit filtern (alles, was nicht Nacht ist)\n",
    "day = energy_train_mit_forecast[~night_condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedingung: Nachtzeiten im Testdatensatz \n",
    "night_condition_test = (\n",
    "    energy_test1_mit_forecast['SolarDownwardRadiation_NCEP_GFS'] == 0\n",
    ")\n",
    "\n",
    "# Filtere Daten für Nachtzeiten im Testdatensatz\n",
    "night_test = energy_test1_mit_forecast[night_condition_test]\n",
    "\n",
    "# Filtere Daten für Tageszeiten im Testdatensatz (alles, was nicht Nacht ist)\n",
    "day_test = energy_test1_mit_forecast[~night_condition_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_night = night\n",
    "df_train_day = day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zu entfernende Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der zu entfernenden Spalten(Training)\n",
    "columns_to_drop = [\"dtm\", \"ref_datetime\", \"Weather Model_x\", \"Weather Model_y\", \"valid_datetime\", \"valid_time\", 'month_year',\n",
    "                   \"CloudCover_NCEP_GFS\",\"CloudCover_DWD_ICON\",  \"Temperature_NCEP_GFS\", \"Solar_capacity_mwp\", \"SolarDownwardRadiation_NCEP_GFS\", \"Temperature_DWD_ICON\"] \n",
    "\n",
    "# Entfernen der definierten Spalten(Training)\n",
    "df_train_night = df_train_night.drop(columns=columns_to_drop)\n",
    "df_train_day = df_train_day.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der zu entfernenden Spalten(Test)\n",
    "columns_to_drop_test = [\"dtm\", \"ref_datetime_x\", \"ref_datetime_y\", \"Weather Model_x\", \"Weather Model_y\", \"valid_datetime\", \"valid_time\", \"month_year\",\n",
    "                    \"CloudCover_NCEP_GFS\",\"CloudCover_DWD_ICON\", \"Temperature_NCEP_GFS\", \"SolarDownwardRadiation_NCEP_GFS\", \"Solar_capacity_mwp\", \"Temperature_DWD_ICON\"]\n",
    "\n",
    "# Entfernen der definierten Spalten(Test)\n",
    "night_test = night_test.drop(columns=columns_to_drop_test)\n",
    "day_test = day_test.drop(columns=columns_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict_night = night_test\n",
    "X_predict_day = day_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"time\"] \n",
    "numerical_features = [\"SolarDownwardRadiation_DWD_ICON\",\"effective_radiation\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen des Labels aus den Daten\n",
    "y_train_night = df_train_night.pop(\"Solar_MWh\")\n",
    "y_train_day = df_train_day.pop(\"Solar_MWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_night = df_train_night\n",
    "X_train_day = df_train_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columntransformer and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ziel: Vorverarbeitung der Daten (kategoriale und numerische Features) in separaten Pipelines.\n",
    "column_trans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # 1. Vorverarbeitung der kategorialen Features:\n",
    "        #    a) Fehlende Werte (NaN) in kategorialen Spalten durch die häufigste Kategorie ersetzen (\"most_frequent\").\n",
    "        #    b) One-hot Encoding für kategoriale Features, um sie in numerische Werte zu transformieren.\n",
    "        (\"onehot\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),  # Fehlende Werte ersetzen\n",
    "            (\"encode\", OneHotEncoder(handle_unknown=\"ignore\"))  # One-hot Encoding\n",
    "        ]), categorical_features),  # Liste der kategorialen Features\n",
    "        \n",
    "        # 2. Vorverarbeitung der numerischen Features:\n",
    "        #    a) Skalierung der numerischen Spalten, um sie standardisiert (Mittelwert=0, Varianz=1) darzustellen.\n",
    "        (\"impute_scale\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"mean\")),  # Handle nulls in numerical\n",
    "            (\"scale\", StandardScaler())  # Skalierung der numerischen Daten\n",
    "        ]), numerical_features)  # Liste der numerischen Features\n",
    "    ],\n",
    "    # Alle anderen Spalten werden unverändert beibehalten (falls vorhanden), da `remainder=\"passthrough\"` angegeben ist.\n",
    "    remainder=\"passthrough\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wendet die oben definierte Vorverarbeitung (column_trans) auf die Daten an.\n",
    "# 2. Nutzt einen GradientBoostingRegressor als Modell für die Vorhersage.\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", column_trans),\n",
    "    (\"model\", GradientBoostingRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametersuche für die Pipeline:\n",
    "# 1. `learning_rate`, `n_estimators`, `max_depth` und `max_features` werden für den GradientBoostingRegressor getestet.\n",
    "# 2. Die Strategie für das Imputing in der OneHot-Encoding-Pipeline wird auf \"most_frequent\" gesetzt.\n",
    "\n",
    "param_grid = {\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"model__n_estimators\": [100, 200, 500],\n",
    "    \"model__max_depth\": [3, 5, 10],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"preprocessing__onehot__impute__strategy\": [\"most_frequent\"],  # Strategy for categorical imputation\n",
    "    \"preprocessing__impute_scale__impute__strategy\": [\"mean\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit für zeitabhängige Kreuzvalidierung:\n",
    "# Ziel: Aufteilen der Daten in Trainings- und Testsets, wobei zukünftige Daten nicht zur Validierung von Modellen \n",
    "# auf vergangenen Daten verwendet werden. Hier wird in 3 Splits unterteilt.\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Einrichtung von GridSearchCV:\n",
    "# 1. `estimator`: Die Pipeline `pipe`, die Vorverarbeitung und Modellierung kombiniert.\n",
    "# 2. `param_grid`: Hyperparameter, die getestet werden sollen.\n",
    "# 3. `scoring`: Bewertungsmetrik (negativer RMSE, da RMSE minimiert werden soll; GridSearchCV maximiert standardmäßig).\n",
    "# 4. `cv`: Die TimeSeriesSplit-Strategie für die Kreuzvalidierung.\n",
    "# 5. `n_jobs=-1`: Nutzt alle verfügbaren CPU-Kerne für paralleles Training.\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=\"neg_root_mean_squared_error\", cv=tscv, n_jobs=-1)\n",
    "\n",
    "# Anpassung von GridSearchCV an die Trainingsdaten (nur für Nachtzeiten):\n",
    "# Sucht die beste Kombination der Hyperparameter im `param_grid`, basierend auf den Trainingsdaten.\n",
    "gs.fit(X_train_night, y_train_night)\n",
    "\n",
    "# Abrufen der besten Parameter und des besten Modells:\n",
    "# 1. `best_params_`: Die beste Kombination der Hyperparameter, die im GridSearchCV gefunden wurde.\n",
    "# 2. `best_estimator_`: Das Modell (Pipeline) mit den optimalen Hyperparametern.\n",
    "gs_best_params = gs.best_params_\n",
    "gs_best_model_night = gs.best_estimator_\n",
    "\n",
    "# Berechnung des RMSE (Root Mean Squared Error) aus dem besten CV-Score:\n",
    "# `gs.best_score_` gibt den negativen RMSE zurück, daher wird dieser invertiert und die Wurzel gezogen.\n",
    "train_rmse = np.sqrt(-gs.best_score_)\n",
    "\n",
    "# Ausgabe der besten Parameter und der besten RMSE (Cross-Validation):\n",
    "print(\"Best Parameters:\", gs.best_params_)  # Zeigt die optimalen Hyperparameter an.\n",
    "print(\"Best RMSE (Cross-Validation):\", -gs.best_score_)  # Zeigt den negativen RMSE aus der besten CV-Kombination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage der Zielvariable 'Solar_MWh' für neue Daten:\n",
    "# 1. `best_model_night`: Das aus GridSearchCV hervorgegangene beste Modell, das mit den Nachtzeit-Daten trainiert wurde.\n",
    "# 2. `X_predict_night`: Der Datensatz mit den Eingabefeatures, für den Vorhersagen gemacht werden sollen.\n",
    "# 3. `.predict`: Nutzt das trainierte Modell, um die Zielvariable 'Solar_MWh' basierend auf den Eingabefeatures vorherzusagen.\n",
    "\n",
    "X_predict_night['Solar_MWh_pred'] = gs_best_model_night.predict(X_predict_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit für zeitabhängige Kreuzvalidierung:\n",
    "# Ziel: Die Daten werden in 3 Splits unterteilt, wobei frühere Daten nicht zur Validierung auf späteren Daten genutzt werden.\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Einrichtung von GridSearchCV:\n",
    "# 1. `estimator`: Die Pipeline `pipe`, die Vorverarbeitung und Modell kombiniert.\n",
    "# 2. `param_grid`: Hyperparameter, die getestet werden sollen.\n",
    "# 3. `scoring`: Bewertungsmetrik (negativer RMSE, da RMSE minimiert werden soll; GridSearchCV maximiert standardmäßig).\n",
    "# 4. `cv`: Die TimeSeriesSplit-Strategie für die Kreuzvalidierung.\n",
    "# 5. `n_jobs=-1`: Nutzt alle verfügbaren CPU-Kerne für paralleles Training.\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=\"neg_root_mean_squared_error\", cv=tscv, n_jobs=-1)\n",
    "\n",
    "# Anpassung von GridSearchCV an die Tageszeit-Trainingsdaten:\n",
    "# Sucht die beste Kombination der Hyperparameter im `param_grid`, basierend auf den Tageszeit-Trainingsdaten.\n",
    "gs.fit(X_train_day, y_train_day)\n",
    "\n",
    "# Abrufen der besten Parameter und des besten Modells:\n",
    "# 1. `best_params_`: Die optimale Kombination der Hyperparameter, die während der GridSearch gefunden wurde.\n",
    "# 2. `best_estimator_`: Das trainierte Modell (Pipeline) mit den optimalen Hyperparametern.\n",
    "best_params = gs.best_params_\n",
    "best_model_day = gs.best_estimator_\n",
    "\n",
    "# Ausgabe der besten Parameter und des besten RMSE (Cross-Validation):\n",
    "# `gs.best_score_` gibt den negativen RMSE zurück. Daher wird dieser invertiert für die Ausgabe.\n",
    "print(\"Best Parameters:\", gs.best_params_)  # Zeigt die optimalen Hyperparameter an.\n",
    "print(\"Best RMSE (Cross-Validation):\", -gs.best_score_)  # Zeigt den negativen RMSE aus der besten CV-Kombination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage der Zielvariable 'Solar_MWh' für neue Tageszeit-Daten:\n",
    "# 1. `best_model_day`: Das aus GridSearchCV hervorgegangene beste Modell, das mit den Tageszeit-Daten trainiert wurde.\n",
    "# 2. `X_predict_day`: Der Datensatz mit den Eingabefeatures, für den Vorhersagen gemacht werden sollen.\n",
    "# 3. `.predict`: Nutzt das trainierte Modell, um die Zielvariable 'Solar_MWh' basierend auf den Eingabefeatures vorherzusagen.\n",
    "\n",
    "X_predict_day['Solar_MWh_pred'] = best_model_day.predict(X_predict_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombiniere Vorhersagedaten:\n",
    "# 1. `pd.concat`: Fügt die beiden DataFrames `X_predict_night` (Vorhersagen für Nachtzeiten) und\n",
    "#    `X_predict_day` (Vorhersagen für Tageszeiten) entlang der Zeilen (axis=0) zusammen.\n",
    "X_predict_combined = pd.concat([X_predict_night, X_predict_day], axis=0)\n",
    "\n",
    "# Sortiere den kombinierten DataFrame nach dem ursprünglichen Index:\n",
    "# Ziel: Sicherstellen, dass die Reihenfolge der Zeilen im kombinierten DataFrame\n",
    "# mit der ursprünglichen Zeitachse oder dem ursprünglichen Index übereinstimmt.\n",
    "X_predict_combined = X_predict_combined.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict_combined.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopieren der Spalte 'Solar_MWh_pred' aus dem kombinierten Vorhersagedatensatz:\n",
    "energy_test1_copy['Solar_MWh_pred'] = X_predict_combined['Solar_MWh_pred'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_test1_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichern der DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "energy_test1_copy.to_pickle(\"energy_test1predict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejoined_data = pd.concat([energy_test1_copy, missing_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer erstellen, um fehlende Werte (NaN) durch den Median zu ersetzen\n",
    "#imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "# Median berechnen und fehlende Werte in 'Solar_MWh_pred' ersetzen\n",
    "#rejoined_data['Solar_MWh_pred'] = imp_median.fit_transform(rejoined_data[['Solar_MWh_pred']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejoined_data.to_pickle(\"energy_test2predict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
